\section{Sampling generation}
%\subsection{Sobol sampling}
Using the Sobol sequence sampling point generation proceeds in a similar way as that described in \Cref{sec:tgo1}. However, rather than only generating an arbitrary number of predefined sampling points we will also consider heuristic methods starting with the minimum amount of sampling points required to triangulate an $n$-dimensional space. For example start with the minimum amount of sampling points to construct an $n$-dimensional simplex and continue sampling while continuously calculating the $\bold{H}_1 (\mathcal{H})$ homology groups of the complex. Using the definitions described in this section the sampling is continued until the growth rate of the approximated homology groups slows appreciably. %This latter method is described in !!.

%\subsection{Minimum triangulation sampling}
%In this publication we also consider another uniform sampling which is generated from an algorithm using the symmetry group $S_n$ of the set $\{1, 2, 3, \dots, $n$\}$. This method is described in more detail Section \ref{sec:triangle}, but first we describe how the triangulation is used to construct the directed simplicial complex $\mathcal{H}$.
%\subsection{Triangulation}

%\section{Simplical constructions}



%\section{Triangulation methods} \label{sec:triangle}
%\subsection{Delaunay triangulation of Sobol points}
%!!TODO: Describe Delaunay triangulation. Provide references to Qhull library etc.
In this publication the Sobol sequenced sampling points are triangulated using Delaunay triangulation as implemented in the SciPy library \cite{scipy}. A major disadvantage to this triangulation scheme is that it does not scale well to higher dimensions since it relies on solving convex hull using the quickhull method developed by \citet{Barber1996}. %!!TODO add references to quickhull method scaling if there are any
There are several possibilities for mitigating this problem. Since the Sobol sequence is deterministic the triangulations can be calculated and stored in a database. For SHGO another possibility whereby the convex hull does not need to be solved by using symmetry generated triangulation was developed. Building on the initial $n$-cube triangulation developed by \citeauthor{paulavivcius2014simplicial} \citep{paulavivcius2014simplicial, Zilinskas2016} and using the symmetry groups $S_n,$ $n = \{1, 2, 3, \dots, $n$\}$ to generate an initial triangulation. Subsequent uniform sampling that ensures a symmetrical triangulation is generated in the next generation of simplices. This is done by an ordering of edges and using the cycle $(123 \dots n-1)$ to ensure that we always split every simplex by a hyperplane that goes through a new (child) vertex on the longest edge of simplex and every other vertex in the parent simplex that does not have incidence on the edge. Figure \ref{fig:triangles} demonstrates the symmetry of this sampling in $n=2$ where the longest edge in the initial triangulation was sampled. Here an iteration is defined as any generation of sub-triangulations that provides a triangulation symmetrical to the initial triangulation. An implementation of this sampling sequence is available at \citet{SHGOpy}.

\begin{figure}
\centerline{\includegraphics[scale=1.0]{./Fig10.pdf}}
{\caption{Triangulation of a unit hypercube shown in 2 dimensions for 4 iterations  \label{fig:triangles} }}
\end{figure}

In this publication we will use both the Sobol and the hypercube triangulation sampling sequences. Sobol provides a more direct comparison to the TGO algorithm while the second sequence is more similar to the DISIMPL-v algorithm. We will refer to the different uses of sampling sequences as SHGO-Sobol and SHGO-Simpl in the experimental results in \Cref{sec:results}.

\section{Invariance and convergence of non-continuous, non-linear optimisation problems with bounded variables}
In this section we again consider \Cref{eq:gen_op}, but now consider the case where $\mathbf{g}$ is non-linear. In addition we allow $f$ to be non-continuous and non-linear. It is still assumed that the variables $\mathbf{x}$ are bounded. Furthermore we assume that there is a feasible solution so that $\Omega \neq \emptyset$ and that there exists at least point in range of $f$ mapped within the domain $\Omega$. We will prove that if the simplicial sampling sequence \citep{SHGOpy} is used, then SHGO will retain the Invariance property of \Cref{theorem:invariance_M}. Secondly convergence of the SHGO algorithm is proved when the number of sampling points tends to infinity.

Before proving these properties we will need to define a new construction to deal with discontinuities in $f$. From \Cref{def:h0} and \Cref{def:h0.5} it is clear that $f$ will only map a subset of feasible domain $\Omega$, therefore only points within the this domain need to be considered. A new construction replacing \Cref{def:h0.5} that considers discontinuities (such as singularities) in the hypersurface of $f$ is now defined:

\begin{definition} \label{def:hnew}
For an objective function $f$, $\mathcal{F}$ is the set of scalar outputs mapped by the objective function $f:\mathcal{P} \rightarrow \mathcal{F}$ for a given sampling set $\mathcal{P} \subseteq \Omega \subseteq \mathbb{R}^n$. If a mapping of a vertex $v_i$ does not exist, then we define the mapping as $f: v_i \rightarrow \infty$.
\end{definition}

Note from \Cref{def:h3} that any vertex $v$, $f(v) = \infty$ that is connected to another vertex in $\Omega$ that maps to a finite value will never be a minimiser.

\begin{theorem} \label{theorem:invariance_n} (Invariance of an adequately sampled simplicial complex $\mathcal{H}$ in a non-convex, non-compact space $\Omega$) For a given non-continuous, non-linear objective function $f$ that is adequately sampled by a sampling set of size $N$. If the cardinality of the minimiser pool extracted from the directed simplex $\mathcal{H}$ is $|\mathcal{M}|$. Then any further increase of the sampling set $N$ will not increase $|\mathcal{M}|$.
\end{theorem}

\begin{proof}
\Cref{theorem:invariance_M} holds for any compact hyperrectangular space $\mathbb{B}_0 = [x_l^1, x_u^1] \times [x_l^2, x_u^2] \times \dots \times [x_l^n, x_u^n]$. Consider a set of subspaces $\mathbb{B}_i \cong \mathbb{B}_0 $ with $\mathbb{B}_i \subseteq \Omega ~\forall i \in \mathbf{I}$. That is, $\mathbb{B}_i$ is any compact, rectangular subspace of $\Omega$ that is homeomorphic to $\mathbb{B}_0$ (which is also homeomorphic to a point) and can, therefore, be shrunk or expanded to arbitrary sizes while retaining compactness. Therefore any triangulation $\mathcal{K}_i$ of $\mathbb{B}_i$ retains the Invariance property from \Cref{theorem:invariance_M}. 

We allow all $\mathbb{B}_i$ to be connected or disconnected subspaces with respect to any other $\mathbb{B}_{j \in I}$ within $\Omega$. Now consider the (mod 2) homology groups $\bold{H}_1(\mathcal{K}_i)$ of $\mathcal{K}_i$. Since the homology groups are abelian groups the rank is additive over arbitrary direct sums:
$$
\operatorname{rank}\left(\bigoplus_{i\in I}\bold{H}_1(\mathcal{K}_i)\right) = \sum_{i \in I}\operatorname{rank}(\bold{H}_1(\mathcal{K}_i))
$$
Therefore the triangulations of both connected and disconnected subspaces $\mathbb{B}_i$ within a possibly non-compact space $\Omega$ will retain the same total rank. After adequate sampling, the rank of $\bold{H}_1(\mathcal{K}_i)$ will not increase by \Cref{theorem:invariance_M}. Any point that is not in $\Omega$ is not connected to any graph structure by \Cref{def:h0} and \Cref{def:h0.5} and therefore cannot increase the rank of any homology group $\bold{H}_1(\mathcal{K}_i)$. Finally any vertex $v_i \in \Omega$ for which $f(v_i)$ does not exist will by \Cref{def:hnew} be mapped to infinity by \Cref{def:hnew}. By \Cref{def:h4}, $v_i$ can not be a minimiser and therefore cannot increase the rank of any homology group $\bold{H}_1(\mathcal{K}_i)$.

We have shown that the total rank of the homology groups triangulated on all connected and disconnected subspaces $\mathbb{B}_i \in \Omega$ will not increase after adequate sampling. It remains to be proven that these subspaces exist within $\Omega$. We adapt the convergence proof used by \citet{Paul2014a} for subdivided simplicial complexes. 
\begin{proposition}
For any point $\mathbf{x} \in \Omega$ and any $\epsilon > 0$ there exists an iteration $k(\epsilon) \ge 1$ and a point $\mathbf{x}_i^k \in \mathcal{H}^n \in \Omega$ such that $\norm{\mathbf{x}_i^k - \mathbf{x}} < \epsilon$.
\end{proposition}
Sampling points $\mathbf{x}_i$ are vertices $\mathcal{H}^0$ belonging to the set of $n$-dimensional simplices $\mathcal{H}^n$. Let $\delta^k_{max}$ be the largest diameter of the largest simplex. Since the subdivision is symmetrical all simplices have the same diameter $\delta^k_{max}$ after every iteration of the complex. At every iteration the diameter will be divided through the longest edge, thus reducing the simplices' volumes. After a sufficiently large number of iterations all simplices will have the diameter smaller than $\epsilon$. Therefore the vertices of the complex will converge to any and all points inside compact subspaces $\mathbb{B}_i$ within $\Omega$. Since we have assumed that $\Omega \neq \emptyset$ this proves the existence of subspaces $\mathbb{B}_i$. 

This concludes the proof of \Cref{theorem:invariance_n}
\end{proof}
From this proof the convergence to a global minimum within $\Omega$, if it exists, also trivially follows by noting that $\mathbb{B}_i$ is homeomorphic to a point and that \Cref{theor:stat} applies to any minimiser in $\mathbb{B}_i$. In practice \Cref{def:hnew} is implemented in \citet{SHGOpy} by using exception handling that can capture any mathematical errors in addition to converting any none float numbers outputted by an objective function to infinity objects.


\section{Theoretical comparison to the DISIMPL algorithm}
The DISIMPL algorithm developed by \citeauthor{Paul2014b} \citep{Paul2014b, paulavivcius2014simplicial, Paul2014a} is based on spatial partitioning of the search space. DISIMPL-v in particular should have a similar initial complex as SHGO-Simpl for box problems since this algorithm samples on the vertices of the simplicial complex (while DISIMPL-c samples at the geometric centre of the simplices which is more appropriate for higher dimensional problems). The graph structure of DISIMPL-v can thus be used to construct the directed complex $\mathcal{H}$ and the homological properties can be calculated and applied. An example of one such application is given in the following paragraph.

At every iteration of the DISIMPL algorithm potentially optimal simplices are selected for refinement by considerations the Lipschitz properties of the optimisation problem. In general a combination of promising simplices with good function evaluations (related to local exploration of the search space) and simplices with larger hypervolumes (related to global exploration of the search space). Gb-DISIMPL \citep{Paul2014a} is a very promising acceleration technique accomplished by switching between a "global phase" and a "usual phase". The global phase is focused on exploring simplices with larger hyper volumes and excludes smaller simplices which are potentially optimal in the usual phase. This technique prevents excessive evaluations near local minima as demonstrated in \citet{Paul2014a}. Local minima can put a "drag" on the progress of refining the minimum because the algorithm selects many neighbouring simplices that are slightly worse on the function values, but also slightly larger in volume. A meta-parameter is used in Gb-DISIMPL to select the simplices to be excluded in the global phase and was shown in \citet{Paul2014a} to be very efficient. However, using knowledge from the directed complex of $\mathcal{H}$, the domain containing these simplices near the local minima could also be identified more explicitly through a Sperner labelling if the function is known to be Lipschitz smooth.

%!!TODO: Also show the proof of the inherent lower limit of maintaining mod 2 information relating the vertices in higher dimensions (~ 10 lines) (NEXT PAPER)

%!!TODO: Mentiond the 3 triangulation schemes of the n-cube ($D_3$ etc.)

%!!TODO: Use subplots for these

%\begin{figure} \label{fig:3dg0}
%\centerline{\includegraphics[scale=0.65]{../6shgo/img/3dgen1.pdf}}
%{\caption{The initial triangulation of the 3-cube generated from the symmetry group $S_3$}}
%\end{figure}

%\begin{figure} \label{fig:3dg2}
%\centerline{\includegraphics[scale=0.65]{../6shgo/img/3dgen2.pdf}}
%{\caption{The second generation of the 3-cube generated from hyperplane cuts using the symmetry groups $S_3$ and $S_2$}}
%\end{figure}

%\begin{figure} \label{fig:3dg10}
%\centerline{\includegraphics[scale=0.65]{../6shgo/img/3dgen10.pdf}}
%{\caption{The tenth generation of the 3-cube generated from hyperplane cuts using the symmetry groups $S_3$ and $S_2$}}
%\end{figure}

%\begin{figure} \label{fig:2dg1}
%\centerline{\includegraphics[scale=0.65]{../6shgo/img/2dgen1.pdf}}
%{\caption{The initial triangulation of the 2-cube generated from the symmetry group $S_2$}}
%\end{figure}

%\begin{figure} \label{fig:2dg2}
%\centerline{\includegraphics[scale=0.65]{../6shgo/img/2dgen2.pdf}}
%{\caption{The second generation of the 2-cube generated from hyperplane cuts using the symmetry groups $S_2$ and $S_1$}}
%\end{figure}

%\begin{figure} \label{fig:3dg10}
%\centerline{\includegraphics[scale=0.65]{../6shgo/img/2dgen10.pdf}}
%{\caption{The tenth generation of the 2-cube generated from hyperplane cuts using the symmetry groups $S_2$ and $S_1$}}
%\end{figure}

%\section{Properties of complex and invariance theorem}
%\section{Homomorphisms to mod 2 homology groups}
%(For function characterisation)



\section{Algorithm implementation}
We consider two modes for the SHGO algorithm. In the first a finite number of sampling points $N$ are specified and sampling is continued until an $\Omega$ set of cardinality $N$ is produced and no further sampling occurs. This method is demonstrated by Algorithm \ref{alg:shgo1}. The main reason for this algorithm is to present a more direct comparison to TGO that can be used in numerical experiments.% we will use Algorithm \ref{alg:shgo1} in the numerical experiments.

For the purposes of global optimisation and local minima exploration Algorithm \ref{alg:shgo2} is more appropriate. By continuously calculating the $\bold{H}_1 (\mathcal{H})$ homology group several termination criteria can be used to end the sampling. For example if the amount of local minima is known the sampling can be terminated once $|\mathcal{M}|$ is large enough. Another example with many possible heuristics is tracking the historical difference in $|\mathcal{M}|$ over $|\mathcal{P}|$ and terminating sampling if $|\mathcal{M}|$ is unchanged after a certain increase in $|\mathcal{P}|$. In optimisation problems where the global minimum is known we can also use the stopping criteria such as the one defined by \citet{Paul2016}. 

\[ pe = 100\% \times \begin{cases} 
       \frac{\min\{\mathcal{F}\} - f^*}{|f^*|}, & f* \neq 0 \\
       \min\{\mathcal{F}\},  & f^* = 0
   \end{cases}
\]

Here $\min\{\mathcal{F}\}$ is the minimum function evaluation obtained including values obtained in the output of the local minimisation step as shown in the algorithm. Whatever termination criterion is used it requires an input $\bold{H}_1 (\mathcal{H})$ or $\min\{\mathcal{F}\}$ and should output a Boolean, we will refer to this function as $\mathbf{TERM}(\bold{H}_1 (\mathcal{H}), \min\{\mathcal{F}\})$ in Algorithm \ref{alg:shgo2}. In the practical implementation of the algorithm the user can also specify a finite number of iterations and/or sampling points. This functionality has been programmed into the $\mathbf{TERM}(\bold{H}_1 (\mathcal{H}), \min\{\mathcal{F}\})$ function.


Open source python implementations of both of these algorithms are available and were published under a MIT compatible license \citep{SHGOpy}.

\begin{algorithm} 
\caption{SHGO finite sampling algorithm}
\label{alg:shgo1}
\begin{algorithmic}[1]
\Procedure{Initialisation}{}
\State \bf{Input} \normalfont an objective function $f$, constraint functions $\mathbf{g}$ and variable bounds and $[\mathbf{l}, \mathbf{u}]^n$.
\State \bf{Input} \normalfont $N$ initial sampling points.
\State Define a sampling sequence that generates a set $\mathcal{X}$ of sampling points in the unit hypercube space $[\mathbf{0}, \mathbf{1}]^n$
\EndProcedure
\Procedure{Initial sampling}{}
\State $\mathcal{P} = \emptyset$
\While{$|\mathcal{P}|$ $<$ $N$}
\State Generate $N - |\mathcal{P}|$ sequential sampling points $\mathcal{X} \subset \mathbb{R}^n$
\State Stretch $\mathcal{X}$ over the lower and upper bounds $[\mathbf{l}, \mathbf{u}]^n$
\State  $\mathcal{P} = \{\mathcal{X}_i ~|~ \mathbf{g}(\mathcal{X}_i) \geq 0, \forall \mathcal{X}_i \in \mathcal{X}\} \cup\mathcal{P}$ 
\Comment (Find $\mathcal{P}$ in the feasible subset $\Omega$ by discarding any points mapped outside the linear constraints $g$ and adding to the current set of $\mathcal{P}$.)
\State Set $\mathcal{X} = \emptyset$
\EndWhile
\State Find $\mathcal{F}$ from the objective function $f: \mathcal{P} \rightarrow \mathcal{F}$
\EndProcedure
\Procedure{Construct directed complex $\mathcal{H}$}{}
\State Calculate $\mathcal{H}$ from $h: \mathcal{P}\rightarrow \mathcal{H}$
\EndProcedure
\Procedure{Construct $\mathcal{M}$}{}
\State Find $\mathcal{M}$ from \Cref{def:h4}.
\EndProcedure
\Procedure{Local minimisation}{}
\State Calculate the approximate local minima of $f$ using a local minimisation routine with the elements of $\mathcal{M}$ as starting points.
\Comment These local minimisations can be performed in parallel.
\EndProcedure
\Procedure{Process return objects}{}
\State Order the final outputs of the minima of $f$ found in the local minimisation step to find the approximate global minimum.
\EndProcedure \\
\Return the approximate global minimum and a list of all the minima found in the local minimisation step.
\end{algorithmic}
\end{algorithm} 





\begin{algorithm} 
\caption{SHGO homology group growth algorithm}
\label{alg:shgo2}
\begin{algorithmic}[1]
\Procedure{Initialisation}{}
\State \bf{Input} \normalfont an objective function $f$, constraint functions $\mathbf{g}$ and variable bounds and $[\mathbf{l}, \mathbf{u}]^n$.
\State \bf{Input} \normalfont $N$ initial sampling points.
\State Define a sampling sequence that generates a set $\mathcal{X}$ of sampling points in the unit hypercube space $[\mathbf{0}, \mathbf{1}]^n$
\State Define the empty set $\mathcal{M}^E = \emptyset$ of vertices evaluated by a local minimisation.
\EndProcedure
\While{$\mathbf{TERM}(\bold{H}_1 (\mathcal{H}), \min\{\mathcal{F}\})$ is False}
\Procedure{Sampling}{}
\State $\mathcal{P} = \emptyset$
\While{$|\mathcal{P}|$ $<$ $N$}
\State Generate $N - |\mathcal{P}|$ sequential sampling points $\mathcal{X} \subset \mathbb{R}^n$
\State Stretch $\mathcal{X}$ over the lower and upper bounds $[\mathbf{l}, \mathbf{u}]^n$
\State  $\mathcal{P} = \{\mathcal{X}_i ~|~ \mathbf{g}(\mathcal{X}_i)  \geq 0, \forall \mathcal{X}_i \in \mathcal{X}\} \cup\mathcal{P}$ 
\Comment (Find $\mathcal{P}$ in the feasible subset $\Omega$ by discarding any points mapped outside the linear constraints $g$ and adding to the current set of $\mathcal{P}$.)
\State Set $\mathcal{X} = \emptyset$
\EndWhile
\State Find $\mathcal{F}$ from the objective function $f: \mathcal{P} \rightarrow \mathcal{F}$ for any new points in $\mathcal{P}$
\EndProcedure
\Procedure{Construct/append  directed complex $\mathcal{H}$}{}
\State Calculate $\mathcal{H}$ from $h: \mathcal{P}\rightarrow \mathcal{H}$  
\Comment (If $\mathcal{H}$ was already constructed new points in $\mathcal{P}$ are incorporated into the triangulation.)
\State Calculate $\bold{H}_1 (\mathcal{H})$
\EndProcedure
\Procedure{Construct $\mathcal{M}$}{}
\State Find $\mathcal{M}$ from \Cref{def:h4}.
\EndProcedure
\Procedure{Local minimisation}{}
\State Calculate the approximate local minima of $f$ using a local minimisation routine with the elements of $\mathcal{M} \setminus \mathcal{M}^E$ as starting points. 
\Comment Process the most promising points first.
\State $\mathcal{M}^E = \mathcal{M}^E \cap \mathcal{M}$  \Comment This excludes the evaluation any element $ v_i \in \mathcal{M}$ that is known to be the only point that in the domain $\partial \textrm{st}(v_j)$ where $v_j$ is known to any point already used as a starting point in Step 27. If any new $ v_i \in \mathcal{M}$ not in $\mathcal{M}^E $ is known to be the only point $\partial \textrm{st}(v_j)$ it can also be excluded.
\State Add the function outputs of the local minimisation routine to $\mathcal{F}$
\EndProcedure
\State Find new value of $\mathbf{TERM}(\bold{H}_1) (\mathcal{H}, \min\{\mathcal{F}\})$
\EndWhile
\Procedure{Process return objects}{}
\State Order the final outputs of the minima of $f$ found in the local minimisation step to find the approximate global minimum.
\EndProcedure \\
\Return the approximate global minimum and a list of all the minima found in the local minimisation step.
\end{algorithmic}
\end{algorithm} 


\chapter{Axially directed topograph}  \label{sec:atgo}
Based on the observations from \Cref{sec:motivation} we develop the ATGO (axially directed topographical global optimisation) algorithm that, for a given sampling set, always uses the optimal number of starting minimisers as defined for one dimensional objective functions without requiring \it{a-priori} \normalfont specification of the $k$ parameter. Here a new graph structure is proposed and attempts are made to directly extend the idea to higher dimensions by connecting every vertex to the nearest vertex in every cartesian axis direction. In Theorem \ref{theorem:iff1} we show that the one dimensional properties of this algorithm does not extend to higher dimensions which finally leads us to the built up complexes in \Cref{sec:shgo}. The main conclusion of this section is that simpler graph structures cannot be used to find locally convex sub-domains of a function in the same way that was accomplished in \Cref{sec:motivation}.

The algorithm proceeds in the same way as TGO described in \Cref{sec:TGO} except for step 2 where a new structure described in Section 4.1 replaces the topograph. 


\section{Axially directed topograph}
Let $\mathcal{F}$ be the set of scalar outputs mapped by the objective function $f:\mathcal{P} \rightarrow \mathcal{F}$ for a given sampling set $\mathcal{P} \subseteq \Omega \subseteq \mathbb{R}^n$. The scalar elements $f_i \in \mathcal{F}$ have one-to-one correspondence with the vector elements $\bold{p}^i \in \mathcal{P}$ where the integer $i \in \{1, 2, 3, \dots, N\}$ indicates the sampling point index. The vector $\bold{p}^i$ in turn has components $x_j^{i}$ where the integer $j \in \{1, 2, 3, \dots, n\}$ indicates the dimension of the scalar value  $\forall i (x^{i}_1,~x^{i}_2,~x^{i}_3,~\dots,~x^{i}_n) \in \bold{p}^i$. 

We wish to construct a graph that is ordered along the coordinate axes, this is done by formally defining the following related partially ordered sets.

\begin{definition} \label{def:indexset}
Given a finite structured set of $N$ feasible ordered sampling points $\mathcal{P} = (\bold{p}^1, \bold{p}^2, \dots, \bold{p}^N )$ with its corresponding objective function outputs $\mathcal{F} = (f^1, f^2, \dots, f^N )  $, the index set of $\mathcal{P}$ is given as the ordered set $\mathcal{I} = ( i = \{1, 2, 3, \dots, N\}, \leq) $
%Given a finite set of $N$ feasible sampling points $P_1, P_2, \dots, P_N \in \mathcal{P}$ with its corresponding objective function outputs $ \mathcal{F}$, the index set of $\mathcal{P}$ is given as the ordered set $\mathcal{I}= ( i = \{1, 2, 3, \dots, N\}, \leq) $.
\end{definition}
Note that the initial ordering of the index set is arbitrary. What's important is that an ordered index set is defined. This ordering will allow us to keep track of any vertex in the graph to its corresponding sampling point in $\mathcal{P}$ so that the corresponding objective function only needs to be evaluated once. Herein the order is taken as the order that is generated by the Sobol sequence. 

\begin{definition}  \label{def:atgo2}
Given a set of feasible sampling points $\mathcal{P} \subseteq \Omega \subseteq \mathbb{R}^n$ define $X_j$ for every dimension $ j \in \{1, 2, 3, \dots, n\}$  as the partially ordered set $X_j = \{\bold{p}^i~|~\forall i (x^i_j < x^{i + 1}_j) \}$.
%Given a set of feasible sampling points $\mathcal{P} \subseteq \Omega \subseteq \mathbb{R}^n$ define $X^j$ for every dimension $ j \in \{1, 2, 3, \dots, n\}$  as the set $\textrm{\normalfont poset} X^j = (\bold{x}_0, \bold{x}_1, \dots, \bold{x}_N)$ where the vectors $\bold{x}_k$ are defined as $\forall j( \{(\bold{x}_0, \bold{x}_1, \dots, \bold{x}_N)~|~(x_{0, j} \le x_{1, j} \le \dots \le x_{k, j}),  x_{k, j} \in P_i \in \mathcal{P} \})$ with $ k \in \{1, 2, 3, \dots, N\}$.
\end{definition}

%!!!We don't need to define $\mathcal{I}$ using the other definitions!!!
%\begin{definition}
%For every dimension $j$, $\mathcal{I}_j$ is the ordered set of integers such the position of the elements $X_j$ correspond to the original index sampling of $ \mathcal{P}$. $\mathcal{I}_j = \{i~|~\forall i (x^i_j < x^{i + 1}_j) \}$  %$\{(i, i, \dots, i) | i \in \mathcal{I} \}$\
%\end{definition}

The definition is demonstrated with the following numerical example:

\paragraph{Example 3} Given set of the first 5 points in the 2-dimensional Sobol sequence bounded by the 2-cube: $$\mathcal{P}~=~\left((0,~0),~(0.5,~0.5),~(0.75,~0.25),~(0.25,~0.75),~(0.375,~0.375)~\right)~\subseteq~[0,~1]~\times~[0,~1] \subseteq~\mathbb{R}^2$$ let $f(x) = x_1^2 + x_2^2$ so that
$$\mathcal{F} = (0,~0.5,~0.625,~0.625,~0.28125)$$
then 
$$X_1 = ((0,~0), (0.25,~0.75), (0.375,~0.375), (0.5,~0.5), (0.75,~0.25))$$
and 
$$X_2 = ((0,~0), (0.75,~0.25), (0.375,~0.375), (0.5,~0.5), (0.25,~0.75))$$
The corresponding index sets are $\mathcal{I}_1 = (1, 4, 5, 2, 3)$ and $\mathcal{I}_2 = (1, 3, 5, 2, 4)$.  

\begin{definition} \label{def:atgo3}
For every dimension $j$, $\mathcal{F}_j$ is the partially ordered set such that the position of the elements $X_j$ correspond to the original index sampling of $ \mathcal{P}$, $\mathcal{F}_j = \{f^{i, k}_j~|~\forall i (x^i_j < x^{i + 1}_j), f^{i, k}_j = f_k \in \mathcal{F}, k \subseteq  \mathcal{I} \}$ 
\end{definition}
%!!! $f^{i, k}_j = f_k \in \mathcal{F} \rightarrow$  This term needs to be more clear !!!
That is the first superscript $i$ of the elements $f^{i, k}$ indicate the ordering in $\mathcal{F}_j$, while the second superscript $k$ indicates the corresponding scalar value of $f^{i, k}$ in $ \mathcal{F}$. Ordering the example we have
$\mathcal{F}_1 = (0,~0.625,~0.28125,~0.5,~0.625)$ and  $\mathcal{F}_2 = (0,~0.625,~0.28125,~0.5,~0.625)$.


\begin{definition}
For every dimension $j$, define the partially ordered sets of cardinality $N$ such that $\mathcal{F}^+_j = \{{f}^{i, k}_j - {f}^{i - 1, k}_j~|~\forall i (x^i_j < x^{i + 1}_j), f^{i, k}_j = f_k \in \mathcal{F}, i = \{1, 2, \dots, N , k \subset \mathcal{I} \} \}$ and $\mathcal{F}^-_j = \{{f}^{i, k}_j - {f}^{i + 1, k}_j ~|~\forall i (x^i_j < x^{i + 1}_j), f^{i, k}_j = f_k \in \mathcal{F}, i = \{0, 1, \dots, N - 1 \}, k \subset \mathcal{I} \}$ \label{def:atgo4}
\end{definition} 
These sets are essentially objective function differences between the sampling points along each dimensional Cartesesian axis. Continuing from the numerical example we have
\begin{align} \nonumber
\mathcal{F}_1^+ =&~ (~0.625, -0.34375, ~0.21875, ~0.125) \\  \nonumber
\mathcal{F}_2^+ =&~ (-0.625, ~0.34375, -0.21875, -0.125) \\  \nonumber
\mathcal{F}_1^- =&~ (~0.625, -0.34375,  ~0.21875, ~0.125) \\  \nonumber
\mathcal{F}_2^- =&~ (-0.625, ~0.34375, -0.21875, -0.125) 
\end{align}

We denote the elements as $ f_j^{+i, k} \in \mathcal{F}_j^+$ and $ f_j^{-i, k} \in \mathcal{F}_j^-$ for every dimension $ j \in \{1, 2, 3, \dots, n\}$, cartesian ordering $i \subseteq \mathcal{I}$ and corresponding sampling point $k \in \mathcal{I}$.  The usefulness of these abstract constructions is apparent in the following definition.

\begin{definition} \label{def:carpool}
For a given sampling set $\mathcal{P}$. The minimiser pool $\mathcal{M}$ is defined as 
$$\mathcal{M} =\mathcal{M}_c  \cup \mathcal{M}_{lb}   \cup \mathcal{M}_{ub} $$
where

$\mathcal{M}_c = \left\{ \bold{p}^i~|~\forall j \left( (f_j^{+i} > 0 )	\land (f_j^{-(i+1)} > 0) \right), i = \{1, 2, 3, \dots, N - 1\} \right\}$

$\mathcal{M}_{lb} = \left\{ \bold{p}^i~|~\forall j \left(f_j^{-i} < 0\right), i = \{0\} \right\} $

$\mathcal{M}_{ub}=  \left\{ \bold{p}^i~|~\forall j \left(f_j^{+i} < 0 \right), i = \{N\} \right\} $

%$\mathcal{M_1} = \left{ \bold{p}^i~|~\forall j \left( (f_j^{+i} > 0 )	\land (f_j^{-(i+1)} > 0) \right), i = \{2, 3, \dots, N - 1\}\right}$ $ \cup  \left{ \bold{p}^i~|~\forall j \left(f_j^{-i} < 0\right), i = \{0\}\right} \cup  \left{ \bold{p}^i~|~\forall j \left(f_j^{+i} < 0 \right), i = \{N\}\right} $
\end{definition}

That is, we simply check the finite difference between sampling points in every cartesian direction. In addition we check if the sampling points on the boundaries are minimisers.

\begin{theorem} \label{theorem:iff1}
The minimiser pool $\mathcal{M}$ from \Cref{def:carpool} always produces a set that is either smaller than or equal to the optimum minimiser pool as defined by \Cref{def:optpool} iff $j = 1$.
\end{theorem}
\begin{proof}
The proof for $j = 1$ follows the same argument from \Cref{sec:motivation}. By Definition \ref{def:indexset}, \ref{def:atgo2} and \ref{def:atgo3} we have the ordering constructed as $\mathcal{P}$ and $\mathcal{F}_1$. If a given point $\bold{p}^i$ is a minimiser with $f_1^{+i} > 0$ and $f_1^{-i} > 0$, then we have by Definition \ref{def:atgo4} $f^i < f^{i - 1}$ and $f^i < f^{i + 1}$, conversely if a given point $\bold{p}^i$ is not a minimiser then either $f_1^{+i} < 0$ or $f_1^{-i} > 0$ so that regardless of the sampling method used and the Euclidean distance between points a minimiser will never be generated for any point that has $\left((f^i > f^{i - 1}) \land (f^i > f^{i + 1} )\right) \lor \left(( f^i < f^{i - 1}) \land (f^i < f^{i + 1} ) \right)$.

If $j > 1$ we have no such guarantee for a higher dimensional locally convex domain. As a counter example consider the set of points $$\mathcal{P} = \left( (0,~0), (0.25,~0.25), (0.75,~0.125), (0.125,~0.75)\right)$$ on the same function as above, the minimiser set produced is $\mathcal{M} = \{(0,~0),~(0.25,~0.25)\}$ which is clearly larger than optimal and will produce the same local minimum. % TODO: Show calculations?
\end{proof}

This unsatisfactory result for higher dimensions could still potentially show good performance for more regular spaced sampling such as grids, however, as we will see in the next section the SHGO algorithm can guarantee that the optimal minimiser set will be produced for any dimension. 
%!! We stated if and only if so also provide proof that for $j>1$!!!
%!! TODO: Provide a visual aid for this proof? !!

%!!TODO: Can construct arrays/matrices, use boolean values to give formula for calculating M!!

\section{Implementation}
Algorithm \ref{alg:atgo} provides a high-level overview of the ATGO algorithm. A Python implementation of this algorithm can be found in \citet{SHGOpy}. 

\begin{algorithm}
\caption{ATGO algorithm}
\label{alg:atgo}
\begin{algorithmic}[1]
\Procedure{Initialisation}{}
\State \bf{Input} \normalfont an objective function $f$, constraint functions $\mathbf{g}$ and variable bounds and $[\mathbf{l}, \mathbf{u}]^n$.
\State \bf{Input} \normalfont $N$ initial sampling points.
\State Define a sampling sequence that generates a set $\mathcal{X}$ of sampling points in the unit hypercube space $[\mathbf{0}, \mathbf{1}]^n$
\EndProcedure
\Procedure{Initial sampling}{}
\State $\mathcal{P} = \emptyset$
\While{$|\mathcal{P}|$ $<$ $N$}
\State Generate $N - |\mathcal{P}|$ sequential sampling points $\mathcal{X} \subset \mathbb{R}^n$
\State Stretch $\mathcal{X}$ over the lower and upper bounds $[\mathbf{l}, \mathbf{u}]^n$
\State  $\mathcal{P} = \{\mathcal{X}_i ~|~ \mathbf{g}(\mathcal{X}_i) \geq 0, \forall \mathcal{X}_i \in \mathcal{X}\} \cup\mathcal{P}$ 
\Comment (Find $\mathcal{P}$ in the feasible subset $\Omega$ by discarding any points mapped outside the linear constraints $g$ and adding to the current set of $\mathcal{P}$.)
\State Set $\mathcal{X} = \emptyset$
\EndWhile
\State Find $\mathcal{F}$ from the objective function $f: \mathcal{P} \rightarrow \mathcal{F}$
\EndProcedure
\Procedure{Construct $\mathcal{M}$}{}
%\State Calculate $\mathcal{F}^+_j$, $\mathcal{F}^-_j$ and $\mathcal{M}$ using Definitions \Cref{def:indexset} through \Cref{def:carpool}.
\State Calculate $\mathcal{M}$ from the sets $\mathcal{P}$ and $\mathcal{F}$ using Definitions \ref{def:atgo2} through \ref{def:carpool}.
\EndProcedure
\Procedure{Local minimisation}{}
\State Calculate the approximate local minima of $f$ using a local minimisation routine with the elements of $\mathcal{M}$ as starting points.
\Comment These local minimisations can be performed in parallel.
\EndProcedure
\Procedure{Process return objects}{}
\State Order the final outputs of the minima of $f$ found in the local minimisation step to find the approximate global minimum.
\EndProcedure \\
\Return the approximate global minimum and a list of all the minima found in the local minimisation step.
\end{algorithmic}
\end{algorithm} 


%\section{Applications in one dimension}\label{sec:atgo_a}
%Consider again the objective function given in Example 1.

%!! NOTE: I believe including figures here would be too much, otherwise we can have a figure showing the minimiser pool growth.

%\section{Applications in two dimensions}\label{sec:atgo_a}
%!! NOTE:  (Show failure in example paraboloidic function, Sobol points too far on the graph are connected providing visual clues as to why it fails, we can recommend experiments with other low discrepancy sequences or regularly spaced grids). If we do this it will be at least 2+ more pages though.


\chapter{Introduction}
\section{Objective function statement and nomenclature}
Consider a general optimisation problem of the form
\begin{align} \nonumber
\underset{\mathbf{x}}{\textrm{min}} ~&f (\mathbf{x}) \\
\textrm{s.t.}~ &\mathbf{g} (\mathbf{x}) \geq 0
\end{align}
The continuous real objective function $f(\mathbf{x})$ of $n$ dimensionality can be either smooth or non-smooth depending on the local minimisation method used. The variables $\mathbf{x}$ are assumed to be bounded. In this publication we mainly consider real, smooth, but not necessarily convex functions with linear constraint functions. In addition it is assumed that the objective function has a finite number of local minima
\begin{equation} \label{eq:objfun}
f : \mathbb{R}^n \rightarrow \mathbb{R}
\end{equation}
$\mathbf{g}$ maps the set of linear constraints
\begin{equation} \label{eq:gcons}
\mathbf{g} :  [\mathbf{l}, \mathbf{u}]^n  \rightarrow \mathbb{R}^m
\end{equation}
for example if lower and upper bounds $l_i$ and $u_i$ are implemented for each variable then we have an initially defined hyperrectangle 
\begin{equation}
\mathbf{x} \in \Omega \subseteq  [\mathbf{l}, \mathbf{u}]^n~=~[l_1,~u_1]~\times~[l_2,~u_2] ~\times~\dots~\times~[l_n,~u_n] \subseteq  \mathbb{R}^n
\end{equation}
 where $\Omega$ is the limited feasible subset excluding points outside the bounds and constraints. 
\begin{equation} \label{eq:omega}
\Omega = \{ \mathbf{x} \in   [\mathbf{l}, \mathbf{u}]^n ~|~ \mathbf{g}_i (\mathbf{x} ) \geq 0, \forall i =1, \dots, m\}
\end{equation}
Since the constraints in $\mathbf{g}$ are linear the set $\Omega$ is always a compact space.

In the development of SHGO several concepts from algebraic and combinatorial topology \cite{Henle1979} are required. The following definition was adapted from \citet[p. 9]{Hatcher2011} 

\begin{definition}
A \textbf{$\mathbf{k}$-simplex} is a set of $n+1$ vertices in a convex polyhedron of dimension $n$. Formally if the $n+1$ points are the $n+1$ standard $n+1$ basis vectors for  $\mathbb{R}^{(n+1)}$. Then the $n$-dimensional $k$-simplex is the set
 $$ 
S^{n}=  \left\{ (t_1 ,\dots , t_{n +1})  \in\mathbb{R}^{n+1} ~|~ \sum^{n +1}_1 t_{n +1}  = 1,  t_i \geq  0  \right\}
$$

%Concretely, if the n points are the n standard
%nbasis vectors for R , then their join is the (n − 1) dimensional simplex
 %∆n−1 =  \{ (t_1 , ···  , t_n)  \in R  n || t1  + ···  +  tn  = 1 and ti ≥  0  }
\end{definition}
For example, a 2-simplex is a triangle and a 3-simplex is a tetrahedron. We will use the following combinatorial definition of a simplicial complex \citep[p. 107]{Hatcher2011}
\begin{definition}
A \textbf{simplicial complex}  $\mathcal{H}$  is a set $\mathcal{H}^0$  of vertices together with sets $\mathcal{H}^n$  of $n$-simplices, which are ($n + 1$)-element subsets of $\mathcal{H}^0$. The only requirement is that each ($k + 1$)-elements subset of the vertices of an $n$-simplex in $\mathcal{H}^n$ is a $k$-simplex, in $\mathcal{H}^k$ .
\end{definition}
Thus each  $n$-simplex  has $n + 1$ distinct vertices, and no other $n$-simplex has this same set of vertices.

In this publication the $\mathcal{H}$ symbol will be used to represent a (finite) simplicial complex rather than the more standard $\Delta $ to avoid confusion with the difference and Laplacian operators common in optimisation. The superscript $\mathcal{H}^k$ represents the subset of $k-$dimensional simplices where for an $n$ dimensional problem the highest dimensional $k-$simplex contains $n + 1$ vertices. Finally we define a $k$-chain\cite{Henle1979}

\begin{definition}
A $\mathbf{k}$-\textbf{chain} is a union of simplices.
\end{definition}
For example a 0-chain is a set of vertices, a 1-chain is a set of edges and a 2-chain is a set of triangles. $C(\mathcal{H}^k)$ denotes a $k-$chain of $k-$simplices. A vertex in $\mathcal{H}^0$ is denoted by $v_i$. If $v_i$ and $v_j$ are two endpoints of a directed edge in $\mathcal{H}^1$ from $v_i$ to $v_j$ then the symbol $\overline{v_i v_j}$ represents the edge so that it is bounded by the $0-$chain $\partial \left( \overline{v_i v_j} \right) = v_j - v_i$ and similarly for an edge directed from $v_j$ to $v_i$, we have, $\partial \left( \overline{v_j v_i} \right) = \partial \left( - \overline{v_i v_j} \right) = v_i - v_j$. Higher dimensional simplices can be represented and directed in a similar manner, for example a triangle consisting of three vertices $v_i, v_j$ and $v_k$ directed as $\overline{v_i v_j v_k}$  has the boundary of directed edges $\partial \left( \overline{v_i v_j v_j} \right)  = \overline{v_i v_j} + \overline{v_j v_k} + \overline{v_j v_i}$.

\section{Multimodal objective functions and local minima mapping}
Non-convex problems are commonly solved using global optimisation methods. One such example is the topographical global optimisation (TGO) method \cite{Henderson2015, Torn1986, Torn1990, Torn1992} which is a clustering algorithm that finds several local minima from which the (probable) global minimum is found. It is often desirable to find all the local minima of the objective function for example in applications such as energy landscape exploration of potential models wherein mapping the local minima of the potential functions can provide valuable insights into the system. Algorithms such as the basin-hopping global optimisation algorithm are typically used to find these points \citep{Wales2015}.




% for example in phase equilibria problems where it is required to map the local minima of the Gibbs energy surface to find all the phases in equilibrium using methods described in \citet{Zhang2011} \cite{Michelsen1982, Michelsen1982a, Mitsos2007, Mitsos, Bollas2009, Bollas2009b}.

The graph extracted from the topographical global optimisation (TGO) \cite{Henderson2015, Torn1986, Torn1990, Torn1992} topograph (as described in \Cref{sec:TGO}) is unsatisfactory in some ways. Primarily because several starting points in the same locally convex domain can be generated even when enough information from the objective function sampling is known to prevent this from occurring. This leads to superfluous function evaluations in the local minimisation step of the algorithm. Contrary to intuition, this problem is exacerbated by increasing the number of initial sampling points used in the algorithm as demonstrated in \Cref{sec:TGO}. This can lead to a very large number of function evaluations required to solve the problem. In particular in multimodal energy surfaces where the local minima can often be located in short distances relative to the search space \cite{Zhang2011} and thus requires a large number of initial sampling to locate all these domains. Some shortcomings in using the TGO method to map local minima are:
\begin{itemize}
\item Geometric information available from the sampling points is being disregarded by the graphs built up using only the Euclidean distance metric.
\item Knowledge of the number and location of local minimisers in a given sampling set is not being used to the full extent.
\item More than one minimiser might be produced in the same locally convex domain and there is no guarantee that a minimiser set produced by TGO will be in the locally convex domains of all local minima even if the number of local minima is known and a minimiser set of this cardinality is produced.
\end{itemize}

By constructing a directed simplicial complex it will be shown that the simplicial homology global optimisation (SHGO) algorithm does not produce superfluous starting points for the class of all Lipschitz smooth functions resulting in more efficient performance for these problems compared to TGO. The directed complex is also used to approximate the homology group of the objective function hypersurface which, using integral homology version of the Invariance Theorem \cite{Henle1979}, allows for efficient mapping of optimisation problems where the number of local minima is known \it{a-priori}\normalfont. %For example in phase equilibria problems where the maximum number of minima is known through the Gibbs phase rule and in certain problems the exact number of phases is known \it{a-priori} \normalfont \cite{Zhang2011}.

\section{Derivative-free methods for Lipschitz optimsation problems}
Both the SHGO and TGO algorithms only make use of function evaluations without requiring the derivatives of objective functions. This makes them applicable to black-box global optimisation problems. A recent review and experimental comparison of 22 derivative-free optimisation algorithms by \citet{Rios2013} concluded that global optimisation solvers solvers such as TOMLAB/MULTI-MIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO perform better, on average, than other derivative-free solvers in terms of solution quality within 2500 function evaluations. Both the TOMLAB/GLCCLUSTER and MCS \cite{Huyer1999} implementations are based on the well-known DIRECT (DIviding RECTangle) algorithm \cite{Jones1993}.
 
The DISIMPL (DIviding SIMPLices) algorithm was recently proposed by \citet{Paul2014b}. The experimental investigation in \cite{Paul2014b} shows that the proposed simplicial algorithm gives very competitive results compared to the DIRECT algorithm. DISIMPL has been extended in \cite{paulavivcius2014simplicial, Paul2014a}. The Gb-DISIMPL (Globally-biased DISIMPL) was compared in \cite{Paul2014a} to the DIRECT and DIRECTl methods in extensive numerical experiments on 800 multidimensional multiextremal test functions.

In a recent adaption of DISIMPL for linearly constrained optimisation problems, Lc-DISIMPL \cite{Paul2016} showed extremely competitive results compared to the PSwarm \cite{Vaz2008} and DIRECT-L1 algorithms \cite{finkel2003direct}. In particular the Lc-DISIMPL-v algorithm was shown to solve the problems in a fewer number of function evaluations on average and was the only algorithm to converge on all of the test problems. In this dissertation both the SHGO and TGO algorithms were tested on the same problem set and the results are compared to the data from \cite{Paul2016} which also contains results on the PSwarm \cite{Vaz2008} and DIRECT-L1 algorithms \cite{finkel2003direct}.

The DISIMPL algorithm is the most similar to SHGO in the sense that both make use of a simplicial complex. DISIMPL uses a simplicial complex in a  spatial partitioning of the initial search space. Since the geometric structure of the two algorithms are related, it is reasonable to expect some theoretical relation of its properties. In particular the graph structure in the DISIMPL-v algorithm \cite{Paul2016} can be used to build the directed simplicial complex used by SHGO. In \Cref{sec:shgo} we also show how some of the same principles developed for SHGO can also be applied in the DISIMPL-v algorithm since the same information is readily available to the algorithm.
 
\section{Overview of this citation}
The TGO method is briefly reviewed in \Cref{sec:TGO} closely following the formalism developed by \citet{Henderson2015}. In \Cref{sec:motivation} we provide numerical examples of TGO which is then used as an informal experimental motivation for extending the algorithm. These two chapters are important for continuity and understanding of the improved features of SHGO, in particular \Cref{def:optpool} which will be used as a performance criterion. In \Cref{sec:atgo} we present the most immediately apparent extension of TGO and illustrate the shortcomings of that approach. The new SHGO method is then formally presented in \Cref{sec:shgo}. In \Cref{sec:results} we provide experimental results of linearly constrained problems comparing the SHGO, TGO, Lc-DISIMPL \cite{Paul2016}, PSwarm \cite{Vaz2008} and DIRECT-L1 \cite{finkel2003direct} algorithms. Furthermore SHGO is compared with the TGO, basinhopping (BH) and differential evolution (DE) global optimisation algorithms over a large selection of black-box problems from the SciPy (\citeauthor*{scipy}, \citeyear{scipy}) global optimisation benchmarking test suite. We conclude with various recommendations for possible further improvements of SHGO.

A very quick introduction to SHGO along with installation instructions can be found on the public website for the project: \href{https://stefan-endres.github.io/shgo/}{https://stefan-endres.github.io/shgo/}